{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting details on weight initialization https://pouannes.github.io/blog/initialization/\n",
    "\n",
    "Use Kaiming method \n",
    "\n",
    "\"The Kaiming paper accordingly suggests to initialize the weights of layer l with a zero-mean Gaussian distribution with a standard deviation of sqrt(s/Nl) , and null biases.\"\n",
    "\n",
    "Nl is the number of neurons in layer l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(number):\n",
    "    \"\"\"\n",
    "    Returns the ReLU of a number\n",
    "    ReLU function is 0 for all negative inputs, and f(x) = x for all x >= 0\n",
    "    \"\"\"\n",
    "    return max(0, number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \"\"\"\n",
    "    Neuron class for neural network\n",
    "    Each neuron is a node in the network\n",
    "    Can be organized into layers with Layer class\n",
    "    Each Neuron has a list of Neurons coming in (the upstream neurons in the network)\n",
    "    and a list of Neurons going out (the downstream neurons in the network)\n",
    "    The weights correspond to the list of input Neurons\n",
    "    ex. a Neuron with 5 upstream neurons will have 5 weights, one for each input\n",
    "    Bias determines a Neurons tendency to be off/on\n",
    "    \"\"\"\n",
    "    def __init__(self, activation=0.5, inNeurons=[], outNeurons=[], weights=[], bias=0):\n",
    "        \"\"\"\n",
    "        Constructor for Neuron class\n",
    "        \"\"\"\n",
    "        self.inNeurons = inNeurons\n",
    "        self.outNeurons = outNeurons\n",
    "        self.activation = activation\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        \n",
    "    def addIn(self, n, weight):\n",
    "        \"\"\"\n",
    "        Adds input neuron with provided weight\n",
    "        Used in Layer class to connect Layers\n",
    "        \"\"\"\n",
    "        self.inNeurons.append(n)\n",
    "        self.weights.append(weight)\n",
    "        \n",
    "    def addOut(self, neuron):\n",
    "        \"\"\"\n",
    "        Adds output neuron\n",
    "        Used in Layer class to connect Layers\n",
    "        \"\"\"\n",
    "        self.outNeurons.append(neuron)\n",
    "    \n",
    "    def getActivation(self):\n",
    "        \"\"\"\n",
    "        Returns activation of Neuron\n",
    "        \"\"\"\n",
    "        return self.activation\n",
    "    \n",
    "    def setActivation(self, a):\n",
    "        \"\"\"\n",
    "        Sets activation of Neuron to a\n",
    "        Returns a\n",
    "        \"\"\"\n",
    "        self.activation = a\n",
    "        return a\n",
    "    \n",
    "    def getBias(self):\n",
    "        \"\"\"\n",
    "        Returns bias of Neuron\n",
    "        \"\"\"\n",
    "        return self.bias\n",
    "    \n",
    "    def setBias(self, b):\n",
    "        \"\"\"\n",
    "        Sets bias of Neuron to b\n",
    "        Returns b\n",
    "        \"\"\"\n",
    "        self.bias = b\n",
    "        return b\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    Layer class for neural network\n",
    "    Layers are collections of neurons\n",
    "    Layers can be linked together to form networks\n",
    "    \"\"\"\n",
    "    def __init__(self, neurons=[], generate=True, size=0):\n",
    "        \"\"\"\n",
    "        Constructor for Layer class\n",
    "        Can input a list of neurons to build layer, or randomly generate\n",
    "        neurons for layer of desired size\n",
    "        \"\"\"\n",
    "        self.neurons = neurons.copy()\n",
    "        print(\"Initializing Layer\")\n",
    "        \n",
    "        #  Generates the neurons for the layer\n",
    "        if generate and len(self.neurons) == 0:\n",
    "            for i in range(size):\n",
    "                self.neurons.append(Neuron())\n",
    "        \n",
    "        print(\"Created layer of size %d\" % len(self.neurons))\n",
    "        \n",
    "        #  Initializes attributes\n",
    "        self.size = size\n",
    "        self.upLayer = None\n",
    "        self.downLayer = None\n",
    "        self.weights = None\n",
    "        self.weightsSet = False\n",
    "        self.desiredOut = []\n",
    "        self.z = []  #  Pre-ReLU activations\n",
    "        \n",
    "        #  Records activations and biases of neurons in layer\n",
    "        self.biases = np.array(list(map(Neuron.getBias, self.neurons)))\n",
    "        self.activations = np.array(list(map(Neuron.getActivation, self.neurons)))\n",
    "    \n",
    "    def setActivations(self, actList):\n",
    "        \"\"\"\n",
    "        Sets activations of neurons in layer to actList\n",
    "        \"\"\"\n",
    "        #  Checks to ensure provided activation list is the appropriate size\n",
    "        if len(actList) == self.size:\n",
    "            for i in range(self.size):\n",
    "                self.neurons[i].setActivation(actList[i])\n",
    "            self.activations = actList\n",
    "        else:\n",
    "            raise Exception(\"Invalid activation list length\")\n",
    "                \n",
    "    \n",
    "    def getActivations(self):\n",
    "        \"\"\"\n",
    "        Returns activations of Neurons in Layer\n",
    "        \"\"\"\n",
    "        return self.activations\n",
    "    \n",
    "    def getBiases(self):\n",
    "        \"\"\"\n",
    "        Returns biases of Neurons in Layer\n",
    "        \"\"\"\n",
    "        return self.biases\n",
    "    \n",
    "    def getSize(self):\n",
    "        \"\"\"\n",
    "        Returns number of Neurons in Layer\n",
    "        \"\"\"\n",
    "        return self.size\n",
    "    \n",
    "    def downstreamConnect(self, down, weights=None):\n",
    "        \"\"\"\n",
    "        Connects neuron layer \"down\" to self \n",
    "        Connection is such that \"down\" is downstream in the neural network\n",
    "        Unless specified, weight matrix is initialized randomly within Kaiming distribution\n",
    "        layer1.upstreamConnect(layer2) is equivalent to layer2.downstreamConnect(layer1)\n",
    "        IMPORTANT NOTE: WEIGHT MATRIX MIGHT NOT PROPERLY FOLLOW KAIMING\n",
    "        INSTEAD THEY SAMPLE RANDOMLY FROM A GAUSSIAN DISTRIBUTION WITH VARIANCE DETERMINED BY KAIMING\n",
    "        MAY CAUSE PROBLEMS WITH SMALL LAYERS, POSSIBLY NEEDS FIX LATER\n",
    "        \"\"\"\n",
    "        upLayerNeurons = self.neurons\n",
    "        downLayerNeurons = down.neurons\n",
    "        \n",
    "        if not weights:\n",
    "            #  Create random weight initialization matrix\n",
    "            #  Weights are picked randomly from gaussian of mean=0 and variance according to Kaiming\n",
    "            weightVariance = np.sqrt(2/len(upLayerNeurons))\n",
    "            weights = np.random.normal(scale=weightVariance, size=(down.getSize(), self.getSize()))\n",
    "            print(\"Created %d by %d weight matrix\" % weights.shape)\n",
    "            down.weightsSet = True\n",
    "            \n",
    "        for d in range(down.getSize()):\n",
    "            for u in range(self.getSize()):\n",
    "                #  Connect all Neurons between the Layers\n",
    "                #  Set weights of Neurons in downstream Layer\n",
    "                upLayerNeurons[u].addOut(downLayerNeurons[d])\n",
    "                downLayerNeurons[d].addIn(upLayerNeurons[u], weights[d, u])\n",
    "        \n",
    "        #  Set weight matrix of downstream Layer and mark up/downstream connections\n",
    "        down.weights = weights  #  This should be a method like updateWeights(), might be useful later\n",
    "        down.upLayer = self\n",
    "        self.downLayer = down\n",
    "        self.update()\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def upstreamConnect(self, up):\n",
    "        \"\"\"\n",
    "        IMPLEMENTATION INCOMPLETE\n",
    "        Connects neuron layer \"up\" to self \n",
    "        Connection is such that \"up\" is upstream in the neural network\n",
    "        layer1.upstreamConnect(layer2) is equivalent to layer2.downstreamConnect(layer1)\n",
    "        \"\"\"\n",
    "    \n",
    "    def updateZ(self):\n",
    "        \"\"\"\n",
    "        Updates Neuron pre-ReLU activations based on weight matrix, \n",
    "        biases and activations of upstream Layer\n",
    "        \"\"\"\n",
    "        #  Only update if weights had been set\n",
    "        if self.weightsSet:\n",
    "            #  Update pre-ReLU activations\n",
    "            self.z = np.matmul(self.weights, self.upLayer.getActivations())\n",
    "            self.z = np.subtract(self.z, self.biases)\n",
    "            \n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Updates Neuron activations based on weight matrix, biases and activations of upstream Layer\n",
    "        \"\"\"\n",
    "        #  Only update if weights had been set\n",
    "        if self.weightsSet:\n",
    "            #  Update pre-ReLU activations\n",
    "            self.z = np.matmul(self.weights, self.upLayer.getActivations())\n",
    "            self.z = np.subtract(self.z, self.biases)\n",
    "            #  Update activations\n",
    "            newActivations = np.array(list(map(relu, self.z)))\n",
    "            self.setActivations(newActivations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    \"\"\"\n",
    "    Neural Network class\n",
    "    A NN consists of Neuron Layers, connected in a linear manner\n",
    "    End Layers act as data input and output\n",
    "    Middle Layers each have upstream and downstream connections\n",
    "    and serve as NN's hidden Layers\n",
    "    \"\"\"\n",
    "    def __init__(self, numLayers, layerSizes):\n",
    "        \"\"\"\n",
    "        Constructor for NN class\n",
    "        numLayers is an integer that determines the number of Layers in the NN\n",
    "        layerSizes is a list or tuple of length numLayers that determines the\n",
    "        number of Neurons in each layer, with layerSizes[0] being the size of the input layer\n",
    "        \"\"\"\n",
    "        assert(numLayers == len(layerSizes))\n",
    "        self.numLayers = numLayers\n",
    "        self.layerSizes = layerSizes\n",
    "        print(\"Creating Neural Network with %d layers\" % numLayers)\n",
    "        self.layers = []\n",
    "        self.desiredOut = []\n",
    "        self.lRate = 0.0002\n",
    "        \n",
    "        #  Create layers and connect them\n",
    "        for i in range(numLayers):\n",
    "            self.layers.append(Layer(size=layerSizes[i]))\n",
    "            if i > 0:\n",
    "                #  No downstream connection for the last layer\n",
    "                self.layers[i-1].downstreamConnect(self.layers[i])\n",
    "            self.layers[i].updateZ()\n",
    "        print(\"Neural Network Initialized\")\n",
    "        \n",
    "    def inputData(self, dataIn, dataOut):\n",
    "        \"\"\"\n",
    "        Sets input layer activations to dataIn if data is the proper size\n",
    "        Saves dataOut as desired output if data is the proper size\n",
    "        Otherwise raises Exception\n",
    "        \"\"\"\n",
    "        if self.layers[0].getSize() == len(dataIn) and self.layers[-1].getSize() == len(dataOut):\n",
    "            self.layers[0].setActivations(dataIn)\n",
    "            self.desiredOut = dataOut\n",
    "            self.layers[-1].desiredOut = dataOut\n",
    "        else:\n",
    "            raise Exception(\"Invalid data size\")\n",
    "            \n",
    "    def getCost(self):\n",
    "        \"\"\"\n",
    "        Returns resulting cost of current NN configuration based on desired output\n",
    "        \"\"\"\n",
    "        return np.sum(np.square(np.subtract(self.layers[-1].getActivations(), self.layers[-1].desiredOut)))\n",
    "    \n",
    "    def backprop(layer):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        #  dCost0/daK(L-1) = dz0(L)/daK(L-1) * da0(L)/dz0(L) * dC0/da0(L)\n",
    "        #  dCost/da(L-1) K = sum dCostJ/daK(L-1)\n",
    "        #  z(L) is pre-ReLU activations, a(L) is activations of Layer L\n",
    "#         size = layer.getSize()\n",
    "        upSize = layer.upLayer.getSize()\n",
    "        gradient = np.ndarray(upSize)\n",
    "#         upAct = layer.upLayer.getActivations()\n",
    "        act = layer.getActivations()\n",
    "        actZPartial = np.where(layer.z >= 0, 1, 0)\n",
    "        #  Iterate through each activation in the previous layer, calculating the partial of the cost function\n",
    "        #  with respect to each one\n",
    "#         for k in range(upSize):\n",
    "#             costPartial = 0\n",
    "#             for j in range(size):              \n",
    "#                 costPartial += 2 * (act[j] - layer.desiredOut[j]) * layer.weights[j][k] * actZPartial[j]\n",
    "#             gradient[k] = costPartial\n",
    "        #  Calculate gradient wrt upstream activations\n",
    "        #  masked is the error between activations and desired activations, masked by the binary actZPartial\n",
    "        #  This masked array can then be matric multiplied with the transpose of weights to get the gradient\n",
    "        masked = np.multiply(np.subtract(act, layer.desiredOut), actZPartial)\n",
    "        gradient = 2 * np.matmul(np.transpose(layer.weights), masked)\n",
    "        #print(\"current activation: %s\" % layer.upLayer.getActivations())\n",
    "        #print(\"activation gradient: %s\" % gradient)\n",
    "        layer.upLayer.desiredOut = np.subtract(layer.upLayer.getActivations(), gradient)\n",
    "        #print(\"new desired activation: %s\" % layer.upLayer.desiredOut)\n",
    "    \n",
    "    def gradientDescentWeights(self, layer):\n",
    "        \"\"\"\n",
    "        Finds the vector for gradient descent of weights of a Layer and subtracts \n",
    "        this vector from that Layer's weights\n",
    "        Returns the vector\n",
    "        \"\"\"\n",
    "        #  dCost/dWeights(L) = dz(L)/dw(L) * da(L)/dz(L) * dC/da(L)\n",
    "        #  z(L) is pre-ReLU activations, a(L) is activations of Layer L\n",
    "        size = layer.getSize()\n",
    "        upSize = layer.upLayer.getSize()\n",
    "        gradient = np.ndarray(shape=(size, upSize))\n",
    "#         upAct = layer.upLayer.getActivations()\n",
    "        upAct = np.reshape(np.array(layer.upLayer.getActivations()), (1, -1))\n",
    "        act = layer.getActivations()\n",
    "        actZPartial = np.where(layer.z >= 0, 1, 0)     \n",
    "        #  Iterate through each weight, calculating the partial of the cost function\n",
    "        #  with respect to each one\n",
    "        \n",
    "        row = np.reshape(np.array(self.lRate * 2 * np.multiply(np.subtract(act, layer.desiredOut), actZPartial)), (-1, 1))\n",
    "        gradient = upAct * row\n",
    "#         print(gradient)\n",
    "#         for k in range(upSize):\n",
    "#             for j in range(size):\n",
    "#                 gradient[j][k] = row[j] * upAct[k]\n",
    "#                 gradient[j][k] = self.lRate * 2 * (act[j] - layer.desiredOut[j]) * upAct[k] * actZPartial[j]\n",
    "\n",
    "#         print(\"current weights: %s\" % layer.weights)\n",
    "#         print(\"weight gradient: %s\" % gradient)\n",
    "        layer.weights = np.subtract(layer.weights, gradient)\n",
    "#         print(\"new weights: %s\" % layer.weights)\n",
    "        \n",
    "    def gradientDescentBiases(layer):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        #  dCost/dBiases(L) = dz(L)/db(L) * da(L)/dz(L) * dC/da(L)\n",
    "        #  z(L) is pre-ReLU activations, a(L) is activations of Layer L\n",
    "        actZPartial = np.where(layer.z >= 0, 1, 0)\n",
    "        size = layer.getSize()\n",
    "        gradient = np.ndarray(size)\n",
    "        \n",
    "        for j in range(size):\n",
    "            act = layer.getActivations()\n",
    "            gradient[j] = 2 * (act[j] - layer.desiredOut[j]) * actZPartial[j]\n",
    "        #print(\"bias gradient: %s\" % gradient)\n",
    "        layer.biases = np.subtract(layer.biases, gradient)\n",
    "        #print(\"new biases: %s\" % layer.biases)\n",
    "    \n",
    "    def showStructure(self):\n",
    "        \"\"\"\n",
    "        Prints out the number of layers in the NN\n",
    "        Prints out the sizes of each layer\n",
    "        \"\"\"\n",
    "        print(\"The network is contains %d layers\" % self.numLayers)\n",
    "        print(\"The layer sizes are %s\" % self.layerSizes)\n",
    "        \n",
    "    def updateActivations(self):\n",
    "        \"\"\"\n",
    "        Updates activations of all Neurons in all Layers in NN according\n",
    "        to their weights and the activations of their upstream Layer\n",
    "        \"\"\"\n",
    "        #  Input layer is skipped, as it has no upstream layer\n",
    "        for layer in self.layers[1:]:\n",
    "            layer.update()\n",
    "            \n",
    "    def showActivations(self):\n",
    "        \"\"\"\n",
    "        Displays activations of each Layer of the NN\n",
    "        \"\"\"\n",
    "        for i in range(self.numLayers):\n",
    "            print(self.layers[i].getActivations())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Neural Network with 4 layers\n",
      "Initializing Layer\n",
      "Created layer of size 100\n",
      "Initializing Layer\n",
      "Created layer of size 50\n",
      "Created 50 by 100 weight matrix\n",
      "Initializing Layer\n",
      "Created layer of size 10\n",
      "Created 10 by 50 weight matrix\n",
      "Initializing Layer\n",
      "Created layer of size 10\n",
      "Created 10 by 10 weight matrix\n",
      "Neural Network Initialized\n",
      "The network is contains 4 layers\n",
      "The layer sizes are [100, 50, 10, 10]\n",
      "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "[0.         0.90066114 0.         0.         0.         0.78307904\n",
      " 0.         0.16923808 1.09836061 0.         0.         0.\n",
      " 0.         0.16705863 0.         0.         0.         0.14028944\n",
      " 0.02213235 0.         0.24800153 0.68793687 0.18383863 0.89799707\n",
      " 0.         1.42191026 0.9940645  0.94817536 0.58339178 0.\n",
      " 0.         0.03420802 0.59032806 0.12130466 0.56503202 1.00852606\n",
      " 0.86368088 0.17959025 0.04704223 0.41658204 0.         0.\n",
      " 0.71498727 0.2904089  0.78650736 0.63493017 0.         0.\n",
      " 0.         0.        ]\n",
      "[0.53363701 0.10767096 0.         0.         0.60018346 0.\n",
      " 0.96971137 0.35077364 0.         0.45276943]\n",
      "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "nn = NN(4, [100, 50, 10, 10])\n",
    "nn.showStructure()\n",
    "nn.showActivations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 84.77616167068481 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b0637d9588>]"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.lRate = 0.00005\n",
    "batchSize = 100\n",
    "epochs = 2000\n",
    "avgs = []\n",
    "\n",
    "start = time.time()\n",
    "for i in range(epochs):\n",
    "    totalCost = 0\n",
    "    for j in range(batchSize):\n",
    "        dataIn = np.random.uniform(size=100)\n",
    "        dataOut = [dataIn[i] for i in range(0, 100, 10)]\n",
    "        nn.inputData(dataIn, dataOut)\n",
    "        nn.updateActivations()\n",
    "        for k in range(nn.numLayers-1, 1, -1):\n",
    "            nn.gradientDescentWeights(nn.layers[k])\n",
    "            NN.backprop(nn.layers[k])\n",
    "        nn.gradientDescentWeights(nn.layers[1])\n",
    "#         nn.gradientDescentWeights(nn.layers[-1])\n",
    "#         NN.backprop(nn.layers[-1])\n",
    "#         nn.gradientDescentWeights(nn.layers[2])\n",
    "#         NN.backprop(nn.layers[2])\n",
    "#         nn.gradientDescentWeights(nn.layers[1])\n",
    "        #print(\"cost: %s\" % nn.getCost())\n",
    "        totalCost += nn.getCost()\n",
    "#     print(totalCost/batchSize)\n",
    "    avgs.append(totalCost/batchSize)\n",
    "duration = time.time() - start\n",
    "\n",
    "print(\"Time: %s seconds\" % duration)\n",
    "plt.plot(list(range(epochs)), avgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7496483116812949, 0.3795383907564551, 0.8043755404404884, 0.5582491266468234, 0.6097291754713159, 0.35969639269389975, 0.3989955881137722, 0.3801805288121395, 0.5789456368887715, 0.36965463967687384]\n",
      "[0.74431419 0.57032154 0.87416323 0.79332334 0.57339332 0.18446189\n",
      " 0.78191813 0.         0.16883238 0.46284372]\n"
     ]
    }
   ],
   "source": [
    "dataIn = np.random.uniform(size=100)\n",
    "dataOut = [dataIn[i] for i in range(0, 100, 10)]\n",
    "nn.inputData(dataIn, dataOut)\n",
    "nn.updateActivations()\n",
    "print(nn.layers[-1].desiredOut)\n",
    "print(nn.layers[-1].getActivations())\n",
    "nn.gradientDescentWeights(nn.layers[-1])\n",
    "#NN.updateDesiredActivationsUp(nn.layers[-1])\n",
    "#NN.gradientDescentWeights(nn.layers[1])\n",
    "#NN.gradientDescentBiases(nn.layers|[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.50992608e-01,  7.71871119e-01, -3.59951378e-01,\n",
       "         1.99940210e-01,  9.11822405e-02,  2.09655759e-01,\n",
       "        -1.81324625e-01, -6.36865342e-02,  6.85858604e-01,\n",
       "        -6.30659440e-02,  2.81986583e-01,  7.58628073e-02,\n",
       "        -5.42183536e-02, -8.95330716e-02,  2.31188873e-01,\n",
       "        -3.65784663e-01,  9.58517783e-02,  1.92803285e-01,\n",
       "        -2.67876313e-01, -3.12493933e-01],\n",
       "       [ 7.23204345e-02,  1.36595975e-01, -1.95444486e-02,\n",
       "        -2.62694884e-01, -6.46078748e-02, -1.46146142e-01,\n",
       "        -3.11278218e-01, -4.23603707e-01,  1.42612290e-01,\n",
       "        -2.26814948e-01, -1.02851194e-01, -1.61624449e-01,\n",
       "         1.01032416e-01, -1.85176934e-01,  4.84538118e-01,\n",
       "         1.43089583e-01, -1.53998347e-01, -5.66920613e-01,\n",
       "        -7.00358434e-01,  1.20744329e-02],\n",
       "       [ 6.25626674e-01,  2.98681553e-02, -4.11871363e-01,\n",
       "        -1.49431732e-01, -7.82318598e-03, -6.02319643e-02,\n",
       "         1.91182643e-01,  2.53430917e-01, -8.69987711e-02,\n",
       "        -1.94663536e-01, -2.58035682e-01,  4.75706572e-02,\n",
       "         8.84969962e-02,  2.77144045e-01,  1.20194552e-01,\n",
       "        -2.02069566e-01,  1.33586923e-01,  1.39759844e-01,\n",
       "         1.97952243e-01,  2.45718509e-01],\n",
       "       [ 8.66510963e-02, -1.92402504e-01,  5.89066247e-02,\n",
       "         3.77472099e-01,  3.39497690e-01,  4.50426647e-02,\n",
       "         6.03739868e-01, -8.55853851e-01,  2.34589061e-01,\n",
       "        -9.36009735e-02,  1.16195157e-01, -1.38520076e-01,\n",
       "        -5.03182808e-02,  2.67975553e-01,  2.92557144e-01,\n",
       "        -2.55033175e-01,  1.83546224e-01, -5.13328694e-02,\n",
       "        -2.57841925e-01, -6.44259550e-02],\n",
       "       [ 2.62667929e-01,  4.87695505e-01,  2.87631917e-01,\n",
       "        -3.28653726e-01, -6.33385276e-01,  7.83561609e-02,\n",
       "         8.61587669e-02, -1.26297081e-01,  9.90063635e-02,\n",
       "        -3.66143534e-01,  1.96138820e-01,  2.89933667e-02,\n",
       "        -5.78612876e-04,  1.12128257e-01, -3.81075058e-01,\n",
       "        -5.92349196e-01, -5.93352379e-01,  3.38748890e-01,\n",
       "        -1.49402088e-01, -6.68784493e-02],\n",
       "       [-1.93555933e-01,  3.12299799e-01,  1.70068246e-01,\n",
       "        -2.85505186e-04,  2.19326308e-01, -3.47308408e-01,\n",
       "        -2.62252240e-01, -3.70718618e-01,  2.50368249e-01,\n",
       "        -1.95306325e-02, -6.01156775e-02,  2.77056359e-02,\n",
       "         4.62243905e-01,  1.23629260e-02,  3.36598361e-01,\n",
       "         3.74931587e-02,  1.90975232e-01,  4.30960750e-01,\n",
       "         1.47139283e-01, -3.27122968e-01],\n",
       "       [-1.49478872e-01,  1.56684058e-01, -3.18062868e-01,\n",
       "         7.29714493e-01, -2.43433297e-01, -1.03981245e-02,\n",
       "         3.57206830e-01, -5.92603473e-01,  2.71889269e-01,\n",
       "         1.64140619e-01, -2.49061735e-01,  2.58627378e-02,\n",
       "         9.80751091e-02,  9.80688042e-02,  4.48663929e-01,\n",
       "         3.40746440e-01, -2.37180520e-01,  1.46779772e-01,\n",
       "         2.82207713e-01,  4.61219146e-01],\n",
       "       [ 6.05496520e-01,  4.82782043e-01,  5.12483017e-01,\n",
       "         1.26824782e-01,  4.77846959e-01,  2.40145293e-01,\n",
       "        -7.52439292e-02, -3.57070721e-01, -1.52197014e-01,\n",
       "        -6.39387509e-02, -2.47356350e-01,  9.69016315e-02,\n",
       "        -4.71248261e-01,  1.73018520e-01, -9.51475319e-02,\n",
       "         1.05912660e-01,  2.35138992e-01, -1.05196550e-01,\n",
       "        -4.75933089e-03,  1.93678004e-01],\n",
       "       [ 2.29589668e-01,  2.30303643e-01, -1.78701239e-02,\n",
       "        -3.14632297e-01, -1.56606930e-01, -7.59870696e-01,\n",
       "         3.28415777e-01,  6.48993991e-02,  5.97445412e-01,\n",
       "         1.94105562e-01,  9.83809788e-02,  5.57051007e-02,\n",
       "         2.63967848e-01,  2.19613390e-02, -2.92606926e-01,\n",
       "        -6.83062885e-02, -6.29373815e-02, -2.08155598e-01,\n",
       "        -8.75504828e-02, -2.61553219e-01],\n",
       "       [-7.65574558e-01, -2.34465692e-01, -1.60966424e-01,\n",
       "        -1.67330156e-01, -4.59157779e-01, -1.51102954e-01,\n",
       "        -1.74909623e-01,  1.34899739e-01, -1.74478447e-01,\n",
       "         5.84943377e-02,  3.46167148e-01,  5.62370509e-02,\n",
       "         4.24244982e-01,  1.64520018e-01, -2.95546829e-02,\n",
       "         1.67481987e-02,  3.53685529e-02, -1.47156112e-01,\n",
       "        -2.76676111e-02,  3.96154629e-01]])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.layers[2].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 1.62528255, 1.33213358, 0.        , 0.        ,\n",
       "       1.42753889, 2.54358913, 0.7513953 , 0.        , 1.57140079,\n",
       "       1.79863713, 0.32281664, 1.57558113, 0.54629096, 0.        ,\n",
       "       0.        , 1.33712701, 1.53911227, 0.        , 0.95400028])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.layers[1].getActivations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Invalid data size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-58870bd71d3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdataOut\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataIn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataIn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataOut\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdateActivations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-e3704711c905>\u001b[0m in \u001b[0;36minputData\u001b[1;34m(self, dataIn, dataOut)\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdesiredOut\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataOut\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid data size\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetCost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Invalid data size"
     ]
    }
   ],
   "source": [
    "dataIn = 2*list(np.linspace(0,0.5,50))\n",
    "dataOut = dataIn\n",
    "\n",
    "nn.inputData(dataIn, dataOut)\n",
    "\n",
    "nn.updateActivations()\n",
    "out = nn.layers[-1].getActivations()\n",
    "\n",
    "print(dataOut)\n",
    "print(out)\n",
    "print(nn.layers[-1].weights[0])\n",
    "print(np.dot(nn.layers[-1].weights[0], dataIn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataIn = [1, 1]*50\n",
    "dataOut = dataIn\n",
    "nn.inputData(dataIn, dataOut)\n",
    "\n",
    "print(\"LAST LAYER\")\n",
    "NN.gradientDescentWeights(nn.layers[-1])\n",
    "#NN.updateDesiredActivationsUp(nn.layers[-1])\n",
    "#NN.gradientDescentBiases(nn.layers[-1])\n",
    "\n",
    "nn.updateActivations()\n",
    "nn.showActivations()\n",
    "print(\"cost: %s\" % nn.getCost())\n",
    "\n",
    "dataIn = [1, 0]*50\n",
    "dataOut = dataIn\n",
    "nn.inputData(dataIn, dataOut)\n",
    "\n",
    "print(\"LAST LAYER\")\n",
    "NN.gradientDescentWeights(nn.layers[-1])\n",
    "#NN.updateDesiredActivationsUp(nn.layers[-1])\n",
    "#NN.gradientDescentBiases(nn.layers[-1])\n",
    "\n",
    "nn.updateActivations()\n",
    "nn.showActivations()\n",
    "print(\"cost: %s\" % nn.getCost())\n",
    "\n",
    "dataIn = [0, 1]*50\n",
    "dataOut = dataIn\n",
    "nn.inputData(dataIn, dataOut)\n",
    "\n",
    "print(\"LAST LAYER\")\n",
    "NN.gradientDescentWeights(nn.layers[-1])\n",
    "#NN.updateDesiredActivationsUp(nn.layers[-1])\n",
    "#NN.gradientDescentBiases(nn.layers[-1])\n",
    "\n",
    "nn.updateActivations()\n",
    "nn.showActivations()\n",
    "print(\"cost: %s\" % nn.getCost())\n",
    "\n",
    "dataIn = [0, 0]*50\n",
    "dataOut = dataIn\n",
    "nn.inputData(dataIn, dataOut)\n",
    "\n",
    "print(\"LAST LAYER\")\n",
    "NN.gradientDescentWeights(nn.layers[-1])\n",
    "#NN.updateDesiredActivationsUp(nn.layers[-1])\n",
    "#NN.gradientDescentBiases(nn.layers[-1])\n",
    "\n",
    "nn.updateActivations()\n",
    "nn.showActivations()\n",
    "print(\"cost: %s\" % nn.getCost())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
